{"/pages/about/": {
    "title": "About",
    "keywords": "Jekyll",
    "url": "/pages/about/",
    "body": "I have been lucky enough to work for over 14 years in analytics. I started my data career in Moscow as a BI engineer working with traditional Oracle Data Warehouse, Informatica, and SAP Business Objects. Later I joined Teradata. It was a very rewarding experience. I instantly got access to a worldwide knowledge base of leaders in the data warehousing space. Later, I realized that it would be boring to spend all my life in Moscow so moved to Montenegro and then immigrated to Canada where I joined Amazon where I spent 5 years. I‚Äôve worked in several teams, including Alexa AI (in Boston) and Customer Behaviour Analytics (in Seattle). I participated in genuinely innovative projects where data is the driving force. I‚Äôve witnessed Big Data and Machine Learning in action at the scale of the world‚Äôs largest company. After Amazon, I worked for 3 years at Microsoft Xbox and Microsoft Azure Data&amp;AI. I actively participated in the development and implementation of Microsoft products for analytics - Synapse, Fabric, and Azure Databricks. Now, I help create innovative analytical solutions, build data teams, and modernize outdated solutions. And of course, I teach those interested all over the world. I recently launched a service in English, surfalytics.com (Surf + Analytics), a global counterpart to datalearn (more about it below). I have written several books on analytics: Jumpstart Snowflake: A Step-by-Step Guide to Modern Cloud Analytics Azure Data Factory Cookbook: Build and manage ETL and ELT pipelines with Microsoft Azure‚Äôs serverless data integration service Azure Data Factory Cookbook: Data engineers guide to build and manage ETL and ELT pipelines with data integration, 2nd Edition Tableau Desktop Certified Associate: Exam Guide: Develop your Tableau skills and prepare for Tableau certification with tips from industry experts Tableau 2019.x Cookbook: Over 115 recipes to build end-to-end analytical solutions using Tableau Mastering Business Intelligence with MicroStrategy: Master Business Intelligence with Microstrategy 10 Learning Hunk SAP Lumira Essentials Since 2021 I have been teaching a Cloud Computing Fundamentals course at the University of Victoria. I have extensive experience in developing data communities and speaking at conferences, meetups, and user groups. I was leading Snowflake, and Tableau user groups in Vancouver and presented myself across North America at local meetups or biggest data conferences. In 2017 I started a Canadian consulting company Rock Your Data with the goal to help companies adopt cloud analytics and migrate to the cloud. However, I realized that my primary passion is building and teaching. In 2019 I started free with Roman project datalearn.ru where I was teaching Data Engineering and Analytics professions based in my home country. Around 20 000 people signed up over 3 years and many people landed their 1st job worldwide. I am thankful for the opportunity to share the knowledge that helped to transform other people‚Äôs lives. All of this helps me stay at the forefront of technology and monitor where the analytics industry is heading. I am the founder of the Rock Your Data consulting firm in North America with a focus on modern cloud analytics."
  },"/pages/contact/": {
    "title": "Contact",
    "keywords": "Jekyll",
    "url": "/pages/contact/",
    "body": "You can find me on Linkedin or text in telegram. You also can join our community here and follow us on LinkedIn. You can book the session discovery session with Dmitry Anoshin to start your journey!üöÄ"
  },"/surfalytics/2023-06-04-Introduction.html": {
    "title": "Introduction",
    "keywords": "surfalytics",
    "url": "/surfalytics/2023-06-04-Introduction.html",
    "body": "The core offering of Surfalytics is a 13-module course that contains everything you need to know about data engineering, business intelligence, and analytics. The course includes both theoretical and practical lessons. They are organized in order of complexity, starting from the simplest and progressing to more advanced topics. Each new piece of information will build upon existing knowledge. The course also provides numerous links to external materials, training, and books. The primary goal is to give you foundational knowledge and empower you to succeed in your data career. Let‚Äôs discuss the requirements and the course further: Requirements First and foremost, you need to determine the requirements for comfortable data work when taking the course. I can identify several key components: Internet access üòÄ Preferably a screen size of 15 inches or larger Ideally, 16 GB of RAM (minimum 8 GB), or else it will lag. No worries, Module 0 will cover the laptops question. Operating systems: Windows and Mac. Linux will also work. The best option is MacOS. To access AWS/Azure/GCP, you might need to enter a credit card number during registration (not before the 4th module). A GitHub account. It will be covered in the Module 0. Ability to use Google and ChatGPT üòÄ Presence on social media to share about the course üòÄ Getting Started with Analytics and Data Engineering - this course is about my job as a data engineering and analytics individual contributor and my 14+ years of experience creating analytical solutions in Canada, the USA, Russia, and Europe. If I were to hire a data engineer or BI engineer, I would want them to have the knowledge and competencies that we‚Äôll cover in the course. The course includes basics like Business Intelligence tools, databases, ETL tools, cloud computing, and much more. Even if you have no experience with data, that won‚Äôt be a hindrance. The first few modules will focus on the basics of analytics and classic tasks: Business Intelligence (reporting, visualization, data warehousing, SQL, Excel, data integration). This will be enough for roles like BI developer, data analyst, etc. Starting from the 5th-6th module, we will delve directly into the work of a Data Engineer, building on the knowledge obtained in the initial stages. The course consists of 13 modules: Module 0: Prerequisites Engineering Tools for Data roles This module will set you up for success in your data career üöÄ. You will learn about the proper setup of your workplace üñ•Ô∏è and the first steps to take before diving into analytics and data engineering üìä. Go to Moodule 0 Module 1: The Role of Analytics and Data Engineering in the Organization Let‚Äôs get acquainted with the subject of study. We‚Äôll find out who the Data Engineer is, what other tech roles exist, what they do, and their other titles. Most importantly, we‚Äôll understand how they help businesses be more efficient and make money. We‚Äôll examine the typical architectures of analytical solutions. Module 2: Databases and SQL We will look at a solution for local analytics. We‚Äôll familiarize ourselves with databases and understand their advantages for working with data compared to Excel/Google Sheets. We‚Äôll practice SQL, set up a database, and upload data to it. Then we‚Äôll use Excel/Google Sheets for data visualization. Module 3: Data Visualization, Dashboards, and Reporting - Business Intelligence (BI) We‚Äôll get acquainted with BI tools and learn how to use Tableau, Looker, and Power BI. We‚Äôll delve into the client and server aspects. We‚Äôll discuss the tasks and theory of data visualization and real examples of BI solution implementations. We‚Äôll also get to know the methodology for creating metrics - Pirate Metrics. Module 4: Data Integration and Creating Data Pipelines As the number of data sources grows, it becomes challenging to manually upload and transform data. ETL solutions are used for these tasks. We will also discuss the difference between ETL and ELT. Furthermore, we‚Äôll explore the market solutions and practice with an Open Source solution, using which we‚Äôll be able to load data into Redshift and automate this process. Module 5: Cloud Computing We will find out what is behind the concept of cloud computing, how it‚Äôs used in the West, and why it‚Äôs so popular. We‚Äôll familiarize ourselves with the analytical solutions of Amazon Web Services and Microsoft Azure. We will look at real-life examples of cloud migration. Module 6: Cloud Data Storage In analytics, the center of the universe is usually the data warehouse or data platform. Typically, this is an analytical solution with MPP architecture, and cloud solutions are often used. We‚Äôll acquaint ourselves with one of the most popular solutions, Amazon Redshift, and learn about other similar platforms. We‚Äôll also discuss case studies of migrating traditional solutions to the cloud. Module 7: Introduction to Apache Spark Apache Spark is one of the most popular tools for Data Engineers. This module is dedicated to acquainting ourselves with Apache Spark and examining its functionality. We will practice creating RDDs and Data Frames, and consider key operations and use cases. Module 8: Creating Big Data Solutions Using Hadoop ecosystem Hadoop is the flagship of Big Data solutions. In this module, we will tackle a problem that cannot be handled by traditional ETL/DW tools, helping you understand the difference between DW and BigData. You will know why we use Hadoop. As a management tool, we will use Spark, which will already be pre-installed on Amazon Elastic Map Reduce. For the exercise, we‚Äôll utilize PySpark to read unstructured logs and extract valuable information from them. Module 9: Introduction to the Data Lake There are many versions of the purpose of a Data Lake and its role in the Analytical ecosystem. In this module, we will familiarize ourselves with the concept of a Data Lake, and its role in the ecosystem, and look at typical architectures for building solutions using a Data Lake. We will use AWS and Azure clouds. Module 10: Getting Started with Streaming This module delves deep into the challenges and solutions associated with data streaming. As data becomes more real-time and voluminous, handling continuous data streams is crucial. Participants will understand the nuances of streaming data, and the importance of timely data processing, and get hands-on experience with tools and technologies that facilitate efficient data streaming. Module 11: Machine Learning Tasks Through the Eyes of a Data Engineer Data Engineering and Machine Learning are interlinked in many modern analytical workflows. This module helps participants view Machine Learning tasks from a data engineering perspective. It covers the challenges of preparing large datasets for training, efficient storage and retrieval systems for ML models, and ensuring that data pipelines are ML-ready. Module 12: Best Practices of a Data Engineer A comprehensive guide to becoming an exemplary data engineer, this module touches upon both the technical and non-technical aspects of the role. From employing DevOps, CI/CD, and Infrastructure as Code (IaC) in data operations to developing soft skills for effective communication and teamwork, this module rounds off with a discussion on potential career trajectories and growth opportunities in the field of data engineering."
  }}
